import concurrent.futures
import requests
import time
import random
import sys
# previo a iniciar esto instala requests
# pip install requests
# Configuraci√≥n del Test
# url local de tu api
BASE_URL = "http://127.0.0.1:5000"
TOTAL_REQUESTS = 30000  # N√∫mero total de peticiones a lanzar
CONCURRENT_THREADS = 1  # N√∫mero de hilos simult√°neos (simulando usuarios)

# cambia a los endpoints de tu api
# # Endpoints a probar (Solo lectura para no llenar la base de datos de basura)
# ENDPOINTS = [
#     "/ordenes",
#     "/historial",
#     "/servicios",
#     "/tipos-bicicleta",
#     "/ordenes?page=1&per_page=100",  # Testear paginaci√≥n pesada
#     "/ordenes?search=a",             # Testear b√∫squeda
#     "/ordenes?estado=Pendiente"      # Testear filtros
# ]

def make_request(url):
    """Realiza una petici√≥n GET y devuelve el resultado."""
    start_time = time.time()
    try:
        response = requests.get(url, timeout=5)
        elapsed_time = time.time() - start_time
        return response.status_code, elapsed_time
    except requests.RequestException as e:
        return None, 0

def run_load_test():
    print(f"üöÄ Iniciando prueba de carga (Stress Test) contra {BASE_URL}")
    print(f"üì° Total Peticiones: {TOTAL_REQUESTS}")
    print(f"üßµ Hilos Simult√°neos: {CONCURRENT_THREADS}")
    print("-" * 50)

    # Verificar que el servidor est√© arriba antes de empezar
    try:
        requests.get(BASE_URL + "/ordenes", timeout=2)
        print("‚úÖ Servidor detectado online. Comenzando ataque de prueba...")
    except:
        print("‚ùå Error: No se pudo conectar al servidor. Aseg√∫rate de que el backend est√© corriendo (puerto 5000).")
        sys.exit(1)

    start_total = time.time()
    
    results = []
    
    # Usar ThreadPoolExecutor para lanzar peticiones concurrentes
    with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENT_THREADS) as executor:
        futures = []
        for _ in range(TOTAL_REQUESTS):
            endpoint = random.choice(ENDPOINTS)
            url = BASE_URL + endpoint
            futures.append(executor.submit(make_request, url))
        
        # Recolectar resultados a medida que completan
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            status, elapsed = future.result()
            results.append((status, elapsed))
            
            # Mostrar progreso cada 100 peticiones
            if (i + 1) % 100 == 0:
                print(f"   ... {i + 1} peticiones completadas")

    total_time = time.time() - start_total
    
    # An√°lisis de resultados
    successful = [r for r in results if r[0] == 200]
    failed = [r for r in results if r[0] != 200]
    avg_time = sum(r[1] for r in successful) / len(successful) if successful else 0
    req_per_sec = len(results) / total_time

    print("-" * 50)
    print("üìä RESULTADOS DE LA PRUEBA")
    print("-" * 50)
    print(f"‚è±Ô∏è  Tiempo Total:       {total_time:.2f} segundos")
    print(f"‚úÖ  Peticiones Exitosas: {len(successful)}")
    print(f"‚ùå  Peticiones Fallidas: {len(failed)}")
    print(f"‚ö°  Promedio Latencia:   {avg_time:.4f} segundos")
    print(f"üöÄ  Rendimiento:         {req_per_sec:.2f} peticiones/segundo")
    print("-" * 50)

    if len(failed) > 0:
        print("‚ö†Ô∏è  ADVERTENCIA: Hubo fallos. El servidor podr√≠a estar saturado.")
    else:
        print("‚úÖ  El servidor soport√≥ la carga correctamente.")

if __name__ == "__main__":
    run_load_test()
